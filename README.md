# Generative-AI-Image-Generation

# AI Food Stylist: Fine-Tuning Stable Diffusion for Professional Food Photography

This project is a deep learning pipeline that fine-tunes a pre-trained Stable Diffusion model to become a specialized generator for high-quality, professional food photography. The model is trained on a curated subset of the Food-101 dataset, with rich, descriptive captions automatically generated by the BLIP model.

##  Key Features

* **LoRA Fine-Tuning:** Efficiently fine-tunes the Stable Diffusion 1.5 model using Low-Rank Adaptation (LoRA).
* **Automated Data Preparation:** A script (`prepare_dataset.py`) that processes a raw classification dataset into a format suitable for generative model training.
* **Rich Caption Generation:** Utilizes the Salesforce BLIP model to create detailed text captions for each image, moving beyond simple class labels to provide rich context for the model to learn from.
* **Configurable Pipeline:** All paths, model IDs, and training parameters are managed in a central `config.py` file for easy modification.

## 🍔 Sample Generations

*Prompt: `pro_food_photo, a vibrant bowl of ramen with a soft-boiled egg, chashu pork, and nori, steam rising, detailed, appetizing`*


*Prompt: `pro_food_photo, a stack of fluffy pancakes with melted butter, dripping maple syrup, and fresh blueberries`*


## Technology Stack

* **Base Model:** `runwayml/stable-diffusion-v1-5`
* **Captioning Model:** `Salesforce/blip-image-captioning-large`
* **Core Frameworks:** PyTorch, Hugging Face `Diffusers`, `Transformers`, `Accelerate`
* **Dataset:** Food-101

---

## Project Structure

├── data/
│   ├── raw/
│   │   └── food-101/
│   └── processed/
│       └── food_for_lora/
├── outputs/
│   ├── models/
│   └── generated_images/
├── scripts/
│   ├── prepare_dataset.py
│   ├── train.py
│   └── inference.py
└── src/
└── config.py

---

## Setup and Installation

1.  **Clone the repository (if applicable)**
    ```bash
    git clone <your-repo-url>
    cd your_project_name
    ```

2.  **Create a Python Environment**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On macOS/Linux
    # venv\Scripts\activate  # On Windows
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Place the Dataset**
    Download and unzip the Food-101 dataset. Place the entire `food-101` folder inside the `data/raw/` directory.

5.  **Configure Paths**
    Open `src/config.py` and ensure the `RAW_FOOD101_ROOT` variable points to the correct path of your dataset if it's not in the default location.

---

## Usage Workflow

The project is run in three main stages:

**1. Data Preparation**
This script reads the raw images, generates captions using BLIP, and saves the processed data.
```bash
python scripts/prepare_dataset.py

**2. Model Training**
This script launches the fine-tuning process using the prepared data.
```bash
python scripts/prepare_dataset.py
